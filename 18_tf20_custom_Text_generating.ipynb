{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text generation with RNN\n",
    "* [source site](https://www.tensorflow.org/beta/tutorials/text/text_generation)\n",
    "* [see also Karpathy's blog](http://karpathy.github.io/2015/05/21/rnn-effectiveness/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_file = tf.keras.utils.get_file('shakespeare.txt', 'https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of text: 1115394 characters\n"
     ]
    }
   ],
   "source": [
    "# Read, then decode for py2 compat.\n",
    "text = open(path_to_file, 'rb').read().decode(encoding='utf-8')\n",
    "# length of text is the number of characters in it\n",
    "print ('Length of text: {} characters'.format(len(text)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "You are all resolved rather to die than to famish?\n",
      "\n",
      "All:\n",
      "Resolved. resolved.\n",
      "\n",
      "First Citizen:\n",
      "First, you know Caius Marcius is chief enemy to the people.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Take a look at the first 250 characters in text\n",
    "print(text[:250])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "65 unique characters\n"
     ]
    }
   ],
   "source": [
    "# The unique characters in the file\n",
    "vocab = sorted(set(text))\n",
    "print ('{} unique characters'.format(len(vocab)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[' ',\n",
       " ',',\n",
       " '.',\n",
       " ':',\n",
       " 'C',\n",
       " 'F',\n",
       " 'M',\n",
       " 'a',\n",
       " 'c',\n",
       " 'e',\n",
       " 'f',\n",
       " 'h',\n",
       " 'i',\n",
       " 'k',\n",
       " 'l',\n",
       " 'm',\n",
       " 'n',\n",
       " 'o',\n",
       " 'p',\n",
       " 'r',\n",
       " 's',\n",
       " 't',\n",
       " 'u',\n",
       " 'w',\n",
       " 'y',\n",
       " 'z']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(set('First Citizen:First, you know Caius Marcius is chief enemy to the people.'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'\\n': 0,\n",
       " ' ': 1,\n",
       " '!': 2,\n",
       " '$': 3,\n",
       " '&': 4,\n",
       " \"'\": 5,\n",
       " ',': 6,\n",
       " '-': 7,\n",
       " '.': 8,\n",
       " '3': 9,\n",
       " ':': 10,\n",
       " ';': 11,\n",
       " '?': 12,\n",
       " 'A': 13,\n",
       " 'B': 14,\n",
       " 'C': 15,\n",
       " 'D': 16,\n",
       " 'E': 17,\n",
       " 'F': 18,\n",
       " 'G': 19,\n",
       " 'H': 20,\n",
       " 'I': 21,\n",
       " 'J': 22,\n",
       " 'K': 23,\n",
       " 'L': 24,\n",
       " 'M': 25,\n",
       " 'N': 26,\n",
       " 'O': 27,\n",
       " 'P': 28,\n",
       " 'Q': 29,\n",
       " 'R': 30,\n",
       " 'S': 31,\n",
       " 'T': 32,\n",
       " 'U': 33,\n",
       " 'V': 34,\n",
       " 'W': 35,\n",
       " 'X': 36,\n",
       " 'Y': 37,\n",
       " 'Z': 38,\n",
       " 'a': 39,\n",
       " 'b': 40,\n",
       " 'c': 41,\n",
       " 'd': 42,\n",
       " 'e': 43,\n",
       " 'f': 44,\n",
       " 'g': 45,\n",
       " 'h': 46,\n",
       " 'i': 47,\n",
       " 'j': 48,\n",
       " 'k': 49,\n",
       " 'l': 50,\n",
       " 'm': 51,\n",
       " 'n': 52,\n",
       " 'o': 53,\n",
       " 'p': 54,\n",
       " 'q': 55,\n",
       " 'r': 56,\n",
       " 's': 57,\n",
       " 't': 58,\n",
       " 'u': 59,\n",
       " 'v': 60,\n",
       " 'w': 61,\n",
       " 'x': 62,\n",
       " 'y': 63,\n",
       " 'z': 64}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "char2idx = {u:i for i, u in enumerate(vocab)}\n",
    "char2idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['\\n', ' ', '!', '$', '&', \"'\", ',', '-', '.', '3', ':', ';', '?',\n",
       "       'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M',\n",
       "       'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z',\n",
       "       'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm',\n",
       "       'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z'],\n",
       "      dtype='<U1')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx2char = np.array(vocab)\n",
    "idx2char"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_as_int = np.array([char2idx[c] for c in text])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 14, 43,\n",
       "       44, 53, 56, 43,  1, 61, 43,  1, 54, 56, 53, 41, 43, 43, 42,  1, 39,\n",
       "       52, 63,  1, 44, 59, 56, 58, 46, 43, 56,  6,  1, 46, 43, 39, 56,  1,\n",
       "       51, 43,  1, 57, 54, 43, 39, 49,  8,  0,  0, 13, 50, 50, 10,  0, 31,\n",
       "       54, 43, 39, 49,  6,  1, 57, 54, 43, 39, 49,  8,  0,  0, 18, 47, 56,\n",
       "       57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 37, 53, 59,  1, 39,\n",
       "       56, 43,  1, 39, 50, 50,  1, 56, 43, 57, 53, 50, 60, 43, 42,  1, 56,\n",
       "       39, 58, 46, 43, 56,  1, 58, 53,  1, 42, 47, 43,  1, 58, 46, 39, 52,\n",
       "        1, 58, 53,  1, 44, 39, 51, 47, 57, 46, 12,  0,  0, 13, 50, 50, 10,\n",
       "        0, 30, 43, 57, 53, 50, 60, 43, 42,  8,  1, 56, 43, 57, 53, 50, 60,\n",
       "       43, 42,  8,  0,  0, 18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43,\n",
       "       52, 10,  0, 18, 47, 56, 57, 58,  6,  1, 63, 53, 59,  1, 49, 52, 53,\n",
       "       61,  1, 15, 39, 47, 59, 57,  1, 25, 39, 56, 41, 47, 59, 57,  1, 47,\n",
       "       57,  1, 41, 46, 47, 43, 44,  1, 43, 52, 43, 51, 63,  1, 58, 53,  1,\n",
       "       58, 46, 43,  1, 54, 43, 53, 54, 50, 43,  8,  0])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_as_int[:250]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  '\\n':   0,\n",
      "  ' ' :   1,\n",
      "  '!' :   2,\n",
      "  '$' :   3,\n",
      "  '&' :   4,\n",
      "  \"'\" :   5,\n",
      "  ',' :   6,\n",
      "  '-' :   7,\n",
      "  '.' :   8,\n",
      "  '3' :   9,\n",
      "  ':' :  10,\n",
      "  ';' :  11,\n",
      "  '?' :  12,\n",
      "  'A' :  13,\n",
      "  'B' :  14,\n",
      "  'C' :  15,\n",
      "  'D' :  16,\n",
      "  'E' :  17,\n",
      "  'F' :  18,\n",
      "  'G' :  19,\n",
      "  ...\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "print('{')\n",
    "for char,_ in zip(char2idx, range(20)):\n",
    "    print('  {:4s}: {:3d},'.format(repr(char), char2idx[char]))\n",
    "print('  ...\\n}') # actually I already see the inside of char2idx dictionary, so it doesn't to be necessary "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'First Citizen' ---- characters mapped to int ---- > [18 47 56 57 58  1 15 47 58 47 64 43 52]\n"
     ]
    }
   ],
   "source": [
    "# Show how the first 13 characters from the text are mapped to integers\n",
    "print ('{} ---- characters mapped to int ---- > {}'.format(repr(text[:13]), text_as_int[:13]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Above codes\n",
    "1. text mining\n",
    "2. sorted the character and made dictionary to match each character to int."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The maximum length sentence we want for a single input in characters\n",
    "seq_length = 100\n",
    "examples_per_epoch = len(text)//seq_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create training examples / targets\n",
    "char_dataset = tf.data.Dataset.from_tensor_slices(text_as_int)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F\n",
      "i\n",
      "r\n",
      "s\n",
      "t\n"
     ]
    }
   ],
   "source": [
    "for i in char_dataset.take(5):\n",
    "    print(idx2char[i.numpy()]) # i is int, thus the code work without .numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'First Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou '\n",
      "'are all resolved rather to die than to famish?\\n\\nAll:\\nResolved. resolved.\\n\\nFirst Citizen:\\nFirst, you k'\n",
      "\"now Caius Marcius is chief enemy to the people.\\n\\nAll:\\nWe know't, we know't.\\n\\nFirst Citizen:\\nLet us ki\"\n",
      "\"ll him, and we'll have corn at our own price.\\nIs't a verdict?\\n\\nAll:\\nNo more talking on't; let it be d\"\n",
      "'one: away, away!\\n\\nSecond Citizen:\\nOne word, good citizens.\\n\\nFirst Citizen:\\nWe are accounted poor citi'\n"
     ]
    }
   ],
   "source": [
    "sequences = char_dataset.batch(seq_length+1, drop_remainder=True)\n",
    "\n",
    "for item in sequences.take(5):\n",
    "    print(repr(''.join(idx2char[item.numpy()]))) # each batch has 101 character, here 505 character will output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "505"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len('First Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou '\n",
    "'are all resolved rather to die than to famish?\\n\\nAll:\\nResolved. resolved.\\n\\nFirst Citizen:\\nFirst, you k'\n",
    "\"now Caius Marcius is chief enemy to the people.\\n\\nAll:\\nWe know't, we know't.\\n\\nFirst Citizen:\\nLet us ki\"\n",
    "\"ll him, and we'll have corn at our own price.\\nIs't a verdict?\\n\\nAll:\\nNo more talking on't; let it be d\"\n",
    "'one: away, away!\\n\\nSecond Citizen:\\nOne word, good citizens.\\n\\nFirst Citizen:\\nWe are accounted poor citi')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<MapDataset shapes: ((100,), (100,)), types: (tf.int64, tf.int64)>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def split_input_target(chunk):\n",
    "    input_text = chunk[:-1]\n",
    "    target_text = chunk[1:]\n",
    "    return input_text, target_text\n",
    "\n",
    "dataset = sequences.map(split_input_target)\n",
    "\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<TakeDataset shapes: ((100,), (100,)), types: (tf.int64, tf.int64)>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input data:  'First Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou'\n",
      "Target data: 'irst Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou '\n"
     ]
    }
   ],
   "source": [
    "for input_example, target_example in  dataset.take(1):\n",
    "  print ('Input data: ', repr(''.join(idx2char[input_example.numpy()])))\n",
    "  print ('Target data:', repr(''.join(idx2char[target_example.numpy()]))) # the reason to make the size batch as 100+1.\n",
    "                                                                         # the size of input and target will be 100."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len('First Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step    0\n",
      "  input: 18 ('F')\n",
      "  expected output: 47 ('i')\n",
      "Step    1\n",
      "  input: 47 ('i')\n",
      "  expected output: 56 ('r')\n",
      "Step    2\n",
      "  input: 56 ('r')\n",
      "  expected output: 57 ('s')\n",
      "Step    3\n",
      "  input: 57 ('s')\n",
      "  expected output: 58 ('t')\n",
      "Step    4\n",
      "  input: 58 ('t')\n",
      "  expected output: 1 (' ')\n"
     ]
    }
   ],
   "source": [
    "for i, (input_idx, target_idx) in enumerate(zip(input_example[:5], target_example[:5])):\n",
    "    print(\"Step {:4d}\".format(i))\n",
    "    print(\"  input: {} ({:s})\".format(input_idx, repr(idx2char[input_idx])))\n",
    "    print(\"  expected output: {} ({:s})\".format(target_idx, repr(idx2char[target_idx])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=65, shape=(100,), dtype=int64, numpy=\n",
       "array([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 14, 43,\n",
       "       44, 53, 56, 43,  1, 61, 43,  1, 54, 56, 53, 41, 43, 43, 42,  1, 39,\n",
       "       52, 63,  1, 44, 59, 56, 58, 46, 43, 56,  6,  1, 46, 43, 39, 56,  1,\n",
       "       51, 43,  1, 57, 54, 43, 39, 49,  8,  0,  0, 13, 50, 50, 10,  0, 31,\n",
       "       54, 43, 39, 49,  6,  1, 57, 54, 43, 39, 49,  8,  0,  0, 18, 47, 56,\n",
       "       57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 37, 53, 59])>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create training batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<BatchDataset shapes: ((64, 100), (64, 100)), types: (tf.int64, tf.int64)>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Batch size\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "# Buffer size to shuffle the dataset\n",
    "# (TF data is designed to work with possibly infinite sequences,\n",
    "# so it doesn't attempt to shuffle the entire sequence in memory. Instead,\n",
    "# it maintains a buffer in which it shuffles elements).\n",
    "BUFFER_SIZE = 10000\n",
    "\n",
    "dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)\n",
    "\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Length of the vocabulary in chars\n",
    "vocab_size = len(vocab) # 65.\n",
    "\n",
    "# The embedding dimension\n",
    "embedding_dim = 256\n",
    "\n",
    "# Number of RNN units\n",
    "rnn_units = 1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(vocab_size, embedding_dim, rnn_units, batch_size):\n",
    "    model = tf.keras.Sequential([\n",
    "        tf.keras.layers.Embedding(vocab_size, embedding_dim,\n",
    "                              batch_input_shape=[batch_size, None]),\n",
    "        tf.keras.layers.LSTM(rnn_units,\n",
    "                        return_sequences=True,\n",
    "                        stateful=True,\n",
    "                        recurrent_initializer='glorot_uniform'),\n",
    "        tf.keras.layers.Dense(vocab_size)\n",
    "        ])\n",
    "    return model\n",
    "model = build_model(\n",
    "  vocab_size = len(vocab),\n",
    "  embedding_dim=embedding_dim,\n",
    "  rnn_units=rnn_units,\n",
    "  batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (64, None, 256)           16640     \n",
      "_________________________________________________________________\n",
      "lstm (LSTM)                  (64, None, 1024)          5246976   \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (64, None, 65)            66625     \n",
      "=================================================================\n",
      "Total params: 5,330,241\n",
      "Trainable params: 5,330,241\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64, 100, 65) # (batch_size, sequence_length, vocab_size)\n"
     ]
    }
   ],
   "source": [
    "for input_example_batch, target_example_batch in dataset.take(1):\n",
    "    example_batch_predictions = model(input_example_batch)\n",
    "    print(example_batch_predictions.shape, \"# (batch_size, sequence_length, vocab_size)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampled_indices = tf.random.categorical(example_batch_predictions[0], num_samples=1)\n",
    "sampled_indices = tf.squeeze(sampled_indices,axis=-1).numpy() # converting multi dimentional array into 1D array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0, 64, 54, 18, 36, 11, 20, 50, 45, 53, 44, 46, 41,  3, 14, 64, 47,\n",
       "       11, 47, 45, 58, 26, 53, 11, 49, 13, 15,  1,  7, 15, 22,  9, 52, 13,\n",
       "       56, 55,  7,  1,  2, 39, 50, 34,  0, 45, 22, 27, 52, 12, 42, 26, 21,\n",
       "        2, 34,  2, 10, 62, 17, 54, 23, 44, 12, 53, 38, 48, 40, 37, 36, 61,\n",
       "       58, 10, 58,  6, 16, 57, 26, 53, 55, 21,  1, 42, 34, 25, 23, 35,  1,\n",
       "       29, 39, 44, 50,  8, 59, 63, 34, 29, 31, 12,  2,  2,  2, 12])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sampled_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: \n",
      " 'love moderately; long love doth so;\\nToo swift arrives as tardy as too slow.\\nHere comes the lady: O, '\n",
      "\n",
      "Next Char Predictions: \n",
      " '\\nzpFX;Hlgofhc$Bzi;igtNo;kAC -CJ3nArq- !alV\\ngJOn?dNI!V!:xEpKf?oZjbYXwt:t,DsNoqI dVMKW Qafl.uyVQS?!!!?'\n"
     ]
    }
   ],
   "source": [
    "print(\"Input: \\n\", repr(\"\".join(idx2char[input_example_batch[0]])))\n",
    "print()\n",
    "print(\"Next Char Predictions: \\n\", repr(\"\".join(idx2char[sampled_indices ]))) # without training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction shape:  (64, 100, 65)  # (batch_size, sequence_length, vocab_size)\n",
      "scalar_loss:       4.1754885\n"
     ]
    }
   ],
   "source": [
    "def loss(labels, logits):\n",
    "    return tf.keras.losses.sparse_categorical_crossentropy(labels, logits, from_logits=True)\n",
    "\n",
    "example_batch_loss  = loss(target_example_batch, example_batch_predictions)\n",
    "print(\"Prediction shape: \", example_batch_predictions.shape, \" # (batch_size, sequence_length, vocab_size)\")\n",
    "print(\"scalar_loss:      \", example_batch_loss.numpy().mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam', loss=loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Directory where the checkpoints will be saved\n",
    "checkpoint_dir = './for_generating_RNN'\n",
    "# Name of the checkpoint files\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\")\n",
    "\n",
    "checkpoint_callback=tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=checkpoint_prefix,\n",
    "    save_weights_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "172/172 [==============================] - 11s 63ms/step - loss: 2.5564\n",
      "Epoch 2/30\n",
      "172/172 [==============================] - 10s 60ms/step - loss: 1.8670\n",
      "Epoch 3/30\n",
      "172/172 [==============================] - 10s 59ms/step - loss: 1.6229\n",
      "Epoch 4/30\n",
      "172/172 [==============================] - 10s 59ms/step - loss: 1.4913\n",
      "Epoch 5/30\n",
      "172/172 [==============================] - 10s 59ms/step - loss: 1.4102\n",
      "Epoch 6/30\n",
      "172/172 [==============================] - 10s 59ms/step - loss: 1.3506\n",
      "Epoch 7/30\n",
      "172/172 [==============================] - 10s 57ms/step - loss: 1.3007\n",
      "Epoch 8/30\n",
      "172/172 [==============================] - 10s 57ms/step - loss: 1.2549\n",
      "Epoch 9/30\n",
      "172/172 [==============================] - 10s 57ms/step - loss: 1.2116\n",
      "Epoch 10/30\n",
      "172/172 [==============================] - 10s 57ms/step - loss: 1.1707\n",
      "Epoch 11/30\n",
      "172/172 [==============================] - 10s 58ms/step - loss: 1.1311\n",
      "Epoch 12/30\n",
      "172/172 [==============================] - 10s 58ms/step - loss: 1.0945\n",
      "Epoch 13/30\n",
      "172/172 [==============================] - 10s 58ms/step - loss: 1.0617\n",
      "Epoch 14/30\n",
      "172/172 [==============================] - 10s 57ms/step - loss: 1.0273\n",
      "Epoch 15/30\n",
      "172/172 [==============================] - 10s 57ms/step - loss: 0.9906\n",
      "Epoch 16/30\n",
      "172/172 [==============================] - 10s 57ms/step - loss: 0.9573\n",
      "Epoch 17/30\n",
      "172/172 [==============================] - 10s 56ms/step - loss: 0.9300\n",
      "Epoch 18/30\n",
      "172/172 [==============================] - 10s 56ms/step - loss: 0.9035\n",
      "Epoch 19/30\n",
      "172/172 [==============================] - 10s 56ms/step - loss: 0.8767\n",
      "Epoch 20/30\n",
      "172/172 [==============================] - 10s 56ms/step - loss: 0.8476\n",
      "Epoch 21/30\n",
      "172/172 [==============================] - 10s 56ms/step - loss: 0.8189\n",
      "Epoch 22/30\n",
      "172/172 [==============================] - 10s 56ms/step - loss: 0.7961\n",
      "Epoch 23/30\n",
      "172/172 [==============================] - 9s 55ms/step - loss: 0.7733\n",
      "Epoch 24/30\n",
      "172/172 [==============================] - 10s 55ms/step - loss: 0.7516\n",
      "Epoch 25/30\n",
      "172/172 [==============================] - 10s 55ms/step - loss: 0.7271\n",
      "Epoch 26/30\n",
      "172/172 [==============================] - 9s 55ms/step - loss: 0.7021\n",
      "Epoch 27/30\n",
      "172/172 [==============================] - 9s 55ms/step - loss: 0.6828\n",
      "Epoch 28/30\n",
      "172/172 [==============================] - 10s 55ms/step - loss: 0.6617\n",
      "Epoch 29/30\n",
      "172/172 [==============================] - 9s 55ms/step - loss: 0.6454\n",
      "Epoch 30/30\n",
      "172/172 [==============================] - 10s 55ms/step - loss: 0.6258\n"
     ]
    }
   ],
   "source": [
    "EPOCHS=30\n",
    "history = model.fit(dataset, epochs=EPOCHS, callbacks=[checkpoint_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64, 100, 65) # (batch_size, sequence_length, vocab_size)\n"
     ]
    }
   ],
   "source": [
    "for input_example_batch, target_example_batch in dataset.take(1):\n",
    "    example_batch_predictions = model(input_example_batch)\n",
    "    print(example_batch_predictions.shape, \"# (batch_size, sequence_length, vocab_size)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampled_indices = tf.random.categorical(example_batch_predictions[0], num_samples=1)\n",
    "sampled_indices = tf.squeeze(sampled_indices,axis=-1).numpy() # converting multi dimentional array into 1D array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([53, 59, 43,  1, 39, 53, 59, 43, 56, 57, 58, 43, 50, 63,  1,  1, 21,\n",
       "       53, 52, 45,  1, 50, 53, 60, 43,  7, 35, 47, 58, 46,  1, 57, 53,  8,\n",
       "        0, 32, 46, 53,  1, 57, 55, 47, 44, 58,  1, 39, 56, 43, 47, 60, 43,\n",
       "       57,  1, 39, 57,  1, 40, 53, 40, 42, 63,  1, 39, 57,  1,  5, 53,  1,\n",
       "        1, 46, 50, 39, 61,  1,  1,  0, 43,  1, 43,  1, 41, 53, 51, 43, 57,\n",
       "        1, 24, 46, 43,  1, 50, 39, 64, 47, 10,  1, 47,  6,  1, 58])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sampled_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: \n",
      " 'love moderately; long love doth so;\\nToo swift arrives as tardy as too slow.\\nHere comes the lady: O, '\n",
      "\n",
      "Next Char Predictions: \n",
      " \"oue aouerstely  Iong love-With so.\\nTho sqift areives as bobdy as 'o  hlaw  \\ne e comes Lhe lazi: i, t\"\n"
     ]
    }
   ],
   "source": [
    "print(\"Input: \\n\", repr(\"\".join(idx2char[input_example_batch[0]])))\n",
    "print()\n",
    "print(\"Next Char Predictions: \\n\", repr(\"\".join(idx2char[sampled_indices ]))) # without training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### make model's input as 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'./for_generating_RNN/ckpt_30'"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.train.latest_checkpoint(checkpoint_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = build_model(vocab_size, embedding_dim, rnn_units, batch_size=1)\n",
    "\n",
    "model.load_weights(tf.train.latest_checkpoint(checkpoint_dir))\n",
    "\n",
    "model.build(tf.TensorShape([1, None]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (1, None, 256)            16640     \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (1, None, 1024)           5246976   \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (1, None, 65)             66625     \n",
      "=================================================================\n",
      "Total params: 5,330,241\n",
      "Trainable params: 5,330,241\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(model, start_string):\n",
    "  # Evaluation step (generating text using the learned model)\n",
    "\n",
    "  # Number of characters to generate\n",
    "  num_generate = 1000\n",
    "\n",
    "  # Converting our start string to numbers (vectorizing)\n",
    "  input_eval = [char2idx[s] for s in start_string]\n",
    "  input_eval = tf.expand_dims(input_eval, 0)\n",
    "\n",
    "  # Empty string to store our results\n",
    "  text_generated = []\n",
    "\n",
    "  # Low temperatures results in more predictable text.\n",
    "  # Higher temperatures results in more surprising text.\n",
    "  # Experiment to find the best setting.\n",
    "  temperature = 1.0\n",
    "\n",
    "  # Here batch size == 1\n",
    "  model.reset_states()\n",
    "  for i in range(num_generate):\n",
    "      predictions = model(input_eval)\n",
    "      # remove the batch dimension\n",
    "      predictions = tf.squeeze(predictions, 0)\n",
    "\n",
    "      # using a categorical distribution to predict the word returned by the model\n",
    "      predictions = predictions / temperature\n",
    "      predicted_id = tf.random.categorical(predictions, num_samples=1)[-1,0].numpy()\n",
    "\n",
    "      # We pass the predicted word as the next input to the model\n",
    "      # along with the previous hidden state\n",
    "      input_eval = tf.expand_dims([predicted_id], 0)\n",
    "\n",
    "      text_generated.append(idx2char[predicted_id])\n",
    "\n",
    "  return (start_string + ''.join(text_generated))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROMEO: YYUCY!CIZEGETCAIQLWYOFATGYYZZTZIACENGCEGVICEYTCHE:\n",
      "\n",
      "\n",
      "First Citizen:\n",
      "It was a judge, let us to him and good.\n",
      "Not I'll resign unto Lucentio.\n",
      "\n",
      "TRANIO:\n",
      "And is this man did to oce for your king?\n",
      "\n",
      "HENRY PERCY:\n",
      "Then pardon me, and so Angelo:\n",
      "Indeed, stark you.\n",
      "\n",
      "Both Citizen:\n",
      "And so did so? Get that bears more sent\n",
      "Un ill spirit's royalthy am I, sir.\n",
      "\n",
      "SAMPSON:\n",
      "Nor aimed on me?\n",
      "I have said, look now comes at the leasing seeks,\n",
      "They made you pardon him, 'tis nour as I,\n",
      "IS at these world goes, and made a sim\n",
      "The most mates; but ignores, and let them nesty--\n",
      "Thou slewing in me to strike? Now, pious sir,\n",
      "Whence come to kill my talk and to your gains.\n",
      "\n",
      "KING RICHARD II:\n",
      "This name me not, nor an under young;\n",
      "But I'll swear this;\n",
      "The cover of the white and as much comfort this fair colours, I\n",
      "Say, 'gainst the other were they trust he sayings\n",
      "To be past to it, and many a looking on's three is she stoop\n",
      "To sunset the city that's to die, in time with pity\n",
      "Than hand it garland in him.\n",
      "\n",
      "MIRANDA:\n",
      "Be away wit\n"
     ]
    }
   ],
   "source": [
    "print(generate_text(model, start_string=u\"ROMEO: \"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[30, 27, 25, 17, 27, 10, 1]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_eval = [char2idx[s] for s in 'ROMEO: ']\n",
    "input_eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=159809, shape=(1, 7), dtype=int32, numpy=array([[30, 27, 25, 17, 27, 10,  1]], dtype=int32)>"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_eval = tf.expand_dims(input_eval, 0)\n",
    "input_eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_result = model(input_eval)\n",
    "sampled_result = tf.random.categorical(sample_result[0], num_samples=1)\n",
    "sampled_result = tf.squeeze(sampled_result,axis=-1).numpy() # converting multi dimentional array into 1D array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'IRPN:\\nI'\n"
     ]
    }
   ],
   "source": [
    "print(repr(\"\".join(idx2char[sampled_result]))) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## advanced: customized training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = build_model(\n",
    "  vocab_size = len(vocab),\n",
    "  embedding_dim=embedding_dim,\n",
    "  rnn_units=rnn_units,\n",
    "  batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Adam()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step(inp, target):\n",
    "    with tf.GradientTape() as tape:\n",
    "        predictions = model(inp)\n",
    "        loss = tf.reduce_mean(\n",
    "              tf.keras.losses.sparse_categorical_crossentropy(\n",
    "                target, predictions, from_logits=True))\n",
    "    grads = tape.gradient(loss, model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
    "\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0717 16:22:53.036100 140075295348544 util.py:244] Unresolved object in checkpoint: (root).optimizer\n",
      "W0717 16:22:53.036699 140075295348544 util.py:244] Unresolved object in checkpoint: (root).optimizer.iter\n",
      "W0717 16:22:53.037014 140075295348544 util.py:244] Unresolved object in checkpoint: (root).optimizer.beta_1\n",
      "W0717 16:22:53.037289 140075295348544 util.py:244] Unresolved object in checkpoint: (root).optimizer.beta_2\n",
      "W0717 16:22:53.037549 140075295348544 util.py:244] Unresolved object in checkpoint: (root).optimizer.decay\n",
      "W0717 16:22:53.037808 140075295348544 util.py:244] Unresolved object in checkpoint: (root).optimizer.learning_rate\n",
      "W0717 16:22:53.038078 140075295348544 util.py:244] Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).layer_with_weights-0.embeddings\n",
      "W0717 16:22:53.038480 140075295348544 util.py:244] Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).layer_with_weights-2.kernel\n",
      "W0717 16:22:53.038883 140075295348544 util.py:244] Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).layer_with_weights-2.bias\n",
      "W0717 16:22:53.039262 140075295348544 util.py:244] Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).layer_with_weights-1.cell.kernel\n",
      "W0717 16:22:53.039611 140075295348544 util.py:244] Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).layer_with_weights-1.cell.recurrent_kernel\n",
      "W0717 16:22:53.039946 140075295348544 util.py:244] Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).layer_with_weights-1.cell.bias\n",
      "W0717 16:22:53.040261 140075295348544 util.py:244] Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).layer_with_weights-0.embeddings\n",
      "W0717 16:22:53.040549 140075295348544 util.py:244] Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).layer_with_weights-2.kernel\n",
      "W0717 16:22:53.040873 140075295348544 util.py:244] Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).layer_with_weights-2.bias\n",
      "W0717 16:22:53.041234 140075295348544 util.py:244] Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).layer_with_weights-1.cell.kernel\n",
      "W0717 16:22:53.041550 140075295348544 util.py:244] Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).layer_with_weights-1.cell.recurrent_kernel\n",
      "W0717 16:22:53.041857 140075295348544 util.py:244] Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).layer_with_weights-1.cell.bias\n",
      "W0717 16:22:53.042150 140075295348544 util.py:252] A checkpoint was restored (e.g. tf.train.Checkpoint.restore or tf.keras.Model.load_weights) but not all checkpointed values were used. See above for specific issues. Use expect_partial() on the load status object, e.g. tf.train.Checkpoint.restore(...).expect_partial(), to silence these warnings, or use assert_consumed() to make the check explicit. See https://www.tensorflow.org/alpha/guide/checkpoints#loading_mechanics for details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Batch 0 Loss 4.1739068031311035\n",
      "Epoch 1 Batch 100 Loss 2.3141682147979736\n",
      "Epoch 1 Loss 2.0871\n",
      "Time taken for 1 epoch 10.562193870544434 sec\n",
      "\n",
      "Epoch 2 Batch 0 Loss 2.0485711097717285\n",
      "Epoch 2 Batch 100 Loss 1.860837697982788\n",
      "Epoch 2 Loss 1.7528\n",
      "Time taken for 1 epoch 8.912753105163574 sec\n",
      "\n",
      "Epoch 3 Batch 0 Loss 1.7017529010772705\n",
      "Epoch 3 Batch 100 Loss 1.650705099105835\n",
      "Epoch 3 Loss 1.5746\n",
      "Time taken for 1 epoch 8.8881094455719 sec\n",
      "\n",
      "Epoch 4 Batch 0 Loss 1.5330157279968262\n",
      "Epoch 4 Batch 100 Loss 1.5272574424743652\n",
      "Epoch 4 Loss 1.4673\n",
      "Time taken for 1 epoch 8.852967977523804 sec\n",
      "\n",
      "Epoch 5 Batch 0 Loss 1.4346429109573364\n",
      "Epoch 5 Batch 100 Loss 1.4540618658065796\n",
      "Epoch 5 Loss 1.3960\n",
      "Time taken for 1 epoch 8.983646392822266 sec\n",
      "\n",
      "Epoch 6 Batch 0 Loss 1.371189832687378\n",
      "Epoch 6 Batch 100 Loss 1.3981536626815796\n",
      "Epoch 6 Loss 1.3407\n",
      "Time taken for 1 epoch 8.802841901779175 sec\n",
      "\n",
      "Epoch 7 Batch 0 Loss 1.319631814956665\n",
      "Epoch 7 Batch 100 Loss 1.352746605873108\n",
      "Epoch 7 Loss 1.2920\n",
      "Time taken for 1 epoch 8.79225206375122 sec\n",
      "\n",
      "Epoch 8 Batch 0 Loss 1.2740063667297363\n",
      "Epoch 8 Batch 100 Loss 1.3104132413864136\n",
      "Epoch 8 Loss 1.2503\n",
      "Time taken for 1 epoch 8.850239038467407 sec\n",
      "\n",
      "Epoch 9 Batch 0 Loss 1.2352187633514404\n",
      "Epoch 9 Batch 100 Loss 1.269396185874939\n",
      "Epoch 9 Loss 1.2079\n",
      "Time taken for 1 epoch 9.076128482818604 sec\n",
      "\n",
      "Epoch 10 Batch 0 Loss 1.1998955011367798\n",
      "Epoch 10 Batch 100 Loss 1.231227993965149\n",
      "Epoch 10 Loss 1.1681\n",
      "Time taken for 1 epoch 9.164854526519775 sec\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Training step\n",
    "EPOCHS = 10\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "  start = time.time()\n",
    "\n",
    "  # initializing the hidden state at the start of every epoch\n",
    "  # initally hidden is None\n",
    "  hidden = model.reset_states()\n",
    "\n",
    "  for (batch_n, (inp, target)) in enumerate(dataset):\n",
    "    loss = train_step(inp, target)\n",
    "\n",
    "    if batch_n % 100 == 0:\n",
    "      template = 'Epoch {} Batch {} Loss {}'\n",
    "      print(template.format(epoch+1, batch_n, loss))\n",
    "\n",
    "  # saving (checkpoint) the model every 5 epochs\n",
    "  if (epoch + 1) % 5 == 0:\n",
    "    model.save_weights(checkpoint_prefix.format(epoch=epoch))\n",
    "\n",
    "  print ('Epoch {} Loss {:.4f}'.format(epoch+1, loss))\n",
    "  print ('Time taken for 1 epoch {} sec\\n'.format(time.time() - start))\n",
    "\n",
    "model.save_weights(checkpoint_prefix.format(epoch=epoch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_2 (Embedding)      (64, None, 256)           16640     \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (64, None, 1024)          5246976   \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (64, None, 65)            66625     \n",
      "=================================================================\n",
      "Total params: 5,330,241\n",
      "Trainable params: 5,330,241\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'./for_generating_RNN/ckpt_9'"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.train.latest_checkpoint(checkpoint_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = build_model(vocab_size, embedding_dim, rnn_units, batch_size=1)\n",
    "\n",
    "model.load_weights(tf.train.latest_checkpoint(checkpoint_dir))\n",
    "\n",
    "model.build(tf.TensorShape([1, None]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_3 (Embedding)      (1, None, 256)            16640     \n",
      "_________________________________________________________________\n",
      "lstm_3 (LSTM)                (1, None, 1024)           5246976   \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (1, None, 65)             66625     \n",
      "=================================================================\n",
      "Total params: 5,330,241\n",
      "Trainable params: 5,330,241\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROMEO: if thou arm'd forth!\n",
      "As we have learn a sepulity of lime towers,\n",
      "And by my father's most I but a dot um.\n",
      "And my shose courtest loves mine evils\n",
      "And philvess eyes and within. What cock bits my swift\n",
      "Quean carce but in place? By me\n",
      "End here for held my heart shame great move's dost.\n",
      "Therefore yourself, he shall;\n",
      "Be remedy; in this report of welcome gottenness,\n",
      "When he shall supply them full tce fight\n",
      "That the sea parleman as a himanement.\n",
      "\n",
      "AUFIDIO:\n",
      "I pray, my sey.\n",
      "\n",
      "First Conspirator,\n",
      "Dody to our bretshorn, frighteous present. An her most sort! I would you cave laughter long shall weep?\n",
      "\n",
      "QUEE Mine but this even news to take what thou artsweer\n",
      "Is true the way to cherity will rate\n",
      "He saking and first follows.\n",
      "\n",
      "GRUMIO:\n",
      "\n",
      "ISABELLA:\n",
      "And weapon he, for she nemesparticils?\n",
      "\n",
      "POMPEY:\n",
      "O; ladple dehite\n",
      "Be pardon.\n",
      "\n",
      "CORDOLANUS:\n",
      "Sir: I can complain the king my due.\n",
      "\n",
      "QUEEN MARGARET:\n",
      "For sadve is le, filest thou, if uny makes a cleaven\n",
      "On so ebjuct: by your laging in your beast,\n",
      "She shall be spent forth, \n"
     ]
    }
   ],
   "source": [
    "print(generate_text(model, start_string=u\"ROMEO: \"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "* RNN can receive varying size of input text.\n",
    "* When you want to train RNN with one characeter, you have to input massive texts with sliding the texts with one character step.\n",
    "* When you want to test the model, you might want to rebuild the model with one batch model. and reload the trained weight."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
